- china can manufacture quickly so once they have all the american AGI secrets
it will be hard to keep up on the deployment & scaling side
- security of startups isn't gonna compare to google infrastructure. when google
  starts getting exploited/hacked you know the startups already have been
- already released research: MoE, RL self play, attention speedups, chinchilla
scaling laws
- if you keep AI closed source then US has a greater lead. its more so the
effort of sustaining that progress gap. if they figure out one more piece of the
puzzle then it makes it exponentially easier to keep up. all they need to put
together are the architectural hints. 
- smart people underestimate foreign espionage because they may not see the
value of investing a bunch of military tech to find out what they are building.
good to understand its not just you building it but also healthy to know its
the most advanced software in the world
- the amount of security you would need in a private company would drastically
slow down progress such that its not really private anymore. its technically
private (information speaking) but there is so much state interference that you
get locally locked down to extreme measures
- a deal with china would be nice but unstable/dangerous if the progress gap is
  small
- make it clear that the US is well ahead in cybersec and algorithmic
advancements, then offer them a deal. not using superintelligence against you
("you get your own piece of the pie")  
- everyone talks about AI development and progress issues at SF parties w/
respect to private AI labs but the real issue is secret government operations
- according to leopold, there will be a phase where superintelligence emerges
and we need to make sure CCP doesn't steal the algos and architectures. during
this volatile period, we would require central intelligence (CIA) and other top
security in government to lock everything down. this is critical for the proper
continuation of our species because if we mess up internationally, it likely
will end terribly. we slow down progress and get less immediate benefit from our
AGIs but thats the price to pay for short-term safety. the idea is that over
time we will pan things out like we did with fission bombs, hydrogen bombs,
fighter jets, etc. 
- easy to get internal rogue researchers so might be smart to limit the amount
of people working on the algorithms leading to digital god. thorough scanning of
employees is required. highly recommend airgapping the high-end clusters /
research facilities.
- leopold finds economic thinking very useful when working on superalignment.
good to keep in mind his educational background of econ degree. he works on
alignment rather than mech interp so its more a economic / historical /
worldview analytic perspective rather than building and fixing neural nets.
empirical thinking rather than coding (still math involved but different type)
### SBF
- prev worked at futurefund w/ SBF (before fraud) trying to put talented people
  to work on hard problems. reinventing philanthropy from first principles
- its easy to give successful CEOs a pass on their behavior due to track record

### OpenAI start
- goal of superalignment initially -> what is the succesor to RLHF (not relying
  on the human feedback part)
- superalignment team stopped because of board events + ilya leaving + 20%
compute commitment

### Board Drama
- fired for leaking at openai: was careful about leaking sensitive internal
plans / info and didn't know the leaking of timelines 2027 AGI was confidential
- HR said it was racist to worry about CCP espionage... lmao
- pulled aside by lawyer and asked about alignment, AGI, etc
- not accidental but seemed like a non-concern to the point where he appeared
"non-forthcoming" for simply not remembering certain shared info to external
researchers
- offered investor equity (~1M) at the cost of signing an NDA (can't talk about
  things like AGI on podcasts)
- the conceived selling point of "building safe AGI" rather than coming across
as the company building chatGPT and business software. maybe this is a good
thing because national security is alarmed about private progress on the most
powerful technology ever built
-

### Research effort issue
- initial question is about why you don't get 100x faster research & discovery
when you have 100x more researchers
- each log scale increment is where you discover a new thing
- research tasks not easy to parallelize
- hard to manage and maintain warp speed progress at larger employee scales
- anthropic doesn't hire 100,000 people with 150+ IQ each because of field
talent and fixed human skill training cost
- "what does it take to be an alec radford?"
- tradeoff between serial speed vs parallel speed (100 million average
researchers VS 1 million brilliant researchers running at 100x speed); likely
won't scale linearly like this due to blocks like ctx length but you get the
idea..
- you can more objectively manage this once you can integrate variable w/
respect to each other (IQ, time, speed)
- working in teams VS optimize everything on the level of solo work (depends
what you are building -> different levels of abstraction for differently
trained AGIs to complete an arbitrary tasks)
- research VS engineering VS managing
- iterating... transforming quantity into quality and vice versa (parallel
copies might be easier because you just optimize GPU kernels)
- ideal goal is to just automate a good researcher (faster testing makes the
researcher smarter recurseively, only limited by fast algo implementation in
wrappers like triton to scale compute easier)

### Con't
- right now about ~0.5 OOMs for algorithmic progress and effective compute
(multiplicatively 1 OOM/yr). OOMs for each progress domain is important to pay
attention to so we can see where the world is going (one super AGI from better
architectures/algos vs 1B junior
devs from effective compute)
- since the OOMs somewhat cancel out per year, you end up with a very little
changing cost/token because inference optimization is totally seperate from the
research and front line optimization (architecture & training steps)
- he highlights these numbers could be absurd so we think using the few 2-3
years of progress to draw short term (1-3 yr) predictions
- alignment -> in the wrong hands (ex: CCP) this is what brainwashing looks like
  so solve alignment so AGIs dont break from CCP manipulation if the weights get
  leaked. 
- generally restrict capabilities at release so that your competitors can't
weaponize it if they acquire the weights. the weights is a more immediate threat
whereas faster progress and architecture / training configuration leaks isn't
immediate and can be used long-term because CCP knows exactly how to make it
work for them
- personal note: notice how all the judgements, conclusions from data, and
predictions are based on the GPT architecture and research on top / closely
surrounding it. the low working prob and potentially high payoff research could
be in finding completely different architectures that perform well at a certain
task different than the way GPTs associate token features w/ attn & mlp
- getting automated resesarchers to work on mech interp / general alignment
somehow... no longer a technical problem but more philisophical digging (slow
down thought and analyze properly like a philosopher but speed up a couple OOMs)
- understanding the daily of a chinese citizen, gov official, manufacturing
exec, AI researchers, etc is important because we need to understand their pov
for safe decision making 
- china could pose good offers to american researchers to share information and
  learn research methods / hacks they didn't already know (makes their progress
  faster so the size of the gap decreases in growth rate)
- consider the little time we might get to deploy these complex mechanisms
nationally (the SF AI people thought about this way more and will take gov
officials lots more time to understand which might be too late)
- what is morminism? -> duty to something bigger than yourself is a force for
good
- 


### AGI investment firm (w/ stripe ceo patrick collinson & friends)
- why is this the thing to do? -> massive investment gain (fastest growing
technology that scales itself like nothing else), being in a position to advise
(push things in the right direction), personal freedom, sequence of bets on the
way to AGI, experience navigating the "the unknowns in the unknowns" quadrant
- coming from an econ background its good to take in the seriousness in leopold
  saying the gains are going to scale far more than modern financial markets
expect
- still early to invest but make sure you know the logistics of how compute /
energy / talent is being drawn
- criticizing e/accs by shitposting about superintel/AGI stuff without thinking
  it through long-term (security aspects, writing stuff down)
