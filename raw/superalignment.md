## Superalignment:
- after some careful thinking, it seems the real problem of "superalignment" is
  understanding what your own long-term goal is. this is an impossibly hard
problem because we don't know what will happen in 50-100 years from now. we can
provide human-assistance to gain the intellectual benefit but also have the
slight human "push" in case things don't go according to plan. the issue with
this is the limit of humans. we don't actually know what the moral solutions are
all the time (given by things like the trolley problem). we don't know the
long-term bloating or cons of our decisions in the extreme long-term (1000s of
years) and could end up in a place where we aren't moral enough to be held
responsible for making such decisions. out of desperation, we may request a
superintelligence to guide us. we are now in a place where decisions and
morality is outsourced to the entities we didn't initially intend to make for
us. if we have a slow takeoff, we should be fine because the difficulty of
morally complex problems should scale at roughly the rate of intelligence (this
is uncertain and vague)
- the real issues don't kick in unless we are giving these blackbox AIs to the
public for relatively cheap compute. people will always find a way to jailbreak
them and we may have a bunch of mini cyber attacks everyday
- tools to evaluation what its thinking out loud vs silently (without directly
saying it) 
- the more risk the more the reward, deploy at low risks (limited modalities if
  possible) and use public interaction/data to improve alignment research
(miniscule alignment bugs)
- the main concern from these articles resides in the "AGI to Superintelligence"
  risk. Just don't scale it as quickly. The only condition which you would
have to scale as fast as possible would be if china has your weights. This is
likely to happen so we have to find a solution that limits what both parties can
do (or maybe there will be a cybersec breakthrough that forces china to steal
research rather than weights)
- first of all, how would we even know if we are at "AGI"? the objective gap
people have drawn seems ridicolus. these are dependent on the evals, the authors
or the evals, and a bunch of surrounding checkboxes we don't even know exist. in
reality I think the models will keep getting better and better until they start
to surpass human intel and become a security threat if the energy infrastructure
can support all our "helpers"
- openai's superalignment question resides in taking the relative intelligence
of models to supervise each other. if gpt-2 can safely supervise gpt-4, then you
continue scaling up. the obvious issue I see here is being able to trust that
gpt-6 was properly supervised by gpt-4 and is thus ready to supervise gpt-8.
- mech interp is a promising approach to understanding the internals of an
architecture such that we completely understand the internals
- 
