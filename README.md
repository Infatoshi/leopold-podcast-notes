## **Leopold w/ Dwarkesh Podcast:**

- China can manufacture quickly so once they have all the American AGI secrets it will be hard to keep up on the deployment & scaling side.
- Security of startups isn't gonna compare to Google infrastructure. When Google starts getting exploited/hacked, you know the startups already have been.
- Already released research: MoE, RL self play, attention speedups, Chinchilla scaling laws.
- If you keep AI closed source then the US has a greater lead. It's more so the effort of sustaining that progress gap. If they figure out one more piece of the puzzle then it makes it exponentially easier to keep up. All they need to put together are the architectural hints.
- Smart people underestimate foreign espionage because they may not see the value of investing a bunch of military tech to find out what they are building. Good to understand it's not just you building it but also healthy to know it's the most advanced software in the world.
- The amount of security you would need in a private company would drastically slow down progress such that it's not really private anymore. It's technically private (information speaking) but there is so much state interference that you get locally locked down to extreme measures.
- A deal with China would be nice but unstable/dangerous if the progress gap is small.
- Make it clear that the US is well ahead in cybersec and algorithmic advancements, then offer them a deal. Not using superintelligence against you ("you get your own piece of the pie").
- Everyone talks about AI development and progress issues at SF parties w/ respect to private AI labs but the real issue is secret government operations.
- According to Leopold, there will be a phase where superintelligence emerges and we need to make sure CCP doesn't steal the algos and architectures. During this volatile period, we would require central intelligence (CIA) and other top security in government to lock everything down. This is critical for the proper continuation of our species because if we mess up internationally, it likely will end terribly. We slow down progress and get less immediate benefit from our AGIs but that's the price to pay for short-term safety. The idea is that over time we will pan things out like we did with fission bombs, hydrogen bombs, fighter jets, etc.
- Easy to get internal rogue researchers so might be smart to limit the number of people working on the algorithms leading to the digital god. Thorough scanning of employees is required. Highly recommend airgapping the high-end clusters/research facilities.
- Leopold finds economic thinking very useful when working on superalignment. Good to keep in mind his educational background of econ degree. He works on alignment rather than mech interp so it's more an economic/historical/worldview analytic perspective rather than building and fixing neural nets. Empirical thinking rather than coding (still math involved but different type).

**SBF**

- Previously worked at FutureFund w/ SBF (before fraud) trying to put talented people to work on hard problems. Reinventing philanthropy from first principles.
- It's easy to give successful CEOs a pass on their behavior due to track record.

**OpenAI start**

- Goal of superalignment initially -> What is the successor to RLHF (not relying on the human feedback part).
- Superalignment team stopped because of board events + Ilya leaving + 20% compute commitment.

**Board Drama**

- Fired for leaking at OpenAI: Was careful about leaking sensitive internal plans/info and didn't know the leaking of timelines 2027 AGI was confidential.
- HR said it was racist to worry about CCP espionage... lmao.
- Pulled aside by lawyer and asked about alignment, AGI, etc.
- Not accidental but seemed like a non-concern to the point where he appeared "non-forthcoming" for simply not remembering certain shared info to external researchers.
- Offered investor equity (~$1M) at the cost of signing an NDA (can't talk about things like AGI on podcasts).
- The conceived selling point of "building safe AGI" rather than coming across as the company building ChatGPT and business software. Maybe this is a good thing because national security is alarmed about private progress on the most powerful technology ever built.

**Research effort issue**

- Initial question is about why you don't get 100x faster research & discovery when you have 100x more researchers.
- Each log scale increment is where you discover a new thing.
- Research tasks not easy to parallelize.
- Hard to manage and maintain warp speed progress at larger employee scales.
- Anthropic doesn't hire 100,000 people with 150+ IQ each because of field talent and fixed human skill training cost.
- "What does it take to be an Alec Radford?"
- Tradeoff between serial speed vs parallel speed (100 million average researchers VS 1 million brilliant researchers running at 100x speed); likely won’t scale linearly due to blocks like ctx length but you get the idea...
- You can more objectively manage this once you can integrate variables w/ respect to each other (IQ, time, speed).
- Working in teams VS optimize everything on the level of solo work (depends on what you are building -> different levels of abstraction for differently trained AGIs to complete arbitrary tasks).
- Research VS engineering VS managing.
- Iterating... transforming quantity into quality and vice versa (parallel copies might be easier because you just optimize GPU kernels).
- Ideal goal is to just automate a good researcher (faster testing makes the researcher smarter recursively, only limited by fast algo implementation in wrappers like Triton to scale compute easier).

**Con't**

- Right now about ~0.5 OOMs for algorithmic progress and effective compute (multiplicatively 1 OOM/yr). OOMs for each progress domain is important to pay attention to so we can see where the world is going (one super AGI from better architectures/algos vs 1B junior devs from effective compute).
- Since the OOMs somewhat cancel out per year, you end up with very little changing cost/token because inference optimization is totally separate from the research and front line optimization (architecture & training steps).
- He highlights these numbers could be absurd so we think using the few 2-3 years of progress to draw short-term (1-3 yr) predictions.
- Alignment -> In the wrong hands (ex: CCP), this is what brainwashing looks like. Solve alignment so AGIs don't break from CCP manipulation if the weights get leaked.
- Generally restrict capabilities at release so that your competitors can't weaponize it if they acquire the weights. The weights are a more immediate threat whereas faster progress and architecture/training configuration leaks aren't immediate and can be used long-term because CCP knows exactly how to make it work for them.
- Personal note: Notice how all the judgments, conclusions from data, and predictions are based on the GPT architecture and research on top/closely surrounding it. The low working prob and potentially high payoff research could be in finding completely different architectures that perform well at a certain task different than the way GPTs associate token features w/ attn & MLP.
- Getting automated researchers to work on mech interp / general alignment somehow... no longer a technical problem but more philosophical digging (slow down thought and analyze properly like a philosopher but speed up a couple OOMs).
- Understanding the daily of a Chinese citizen, gov official, manufacturing exec, AI researchers, etc is important because we need to understand their POV for safe decision-making.
- China could pose good offers to American researchers to share information and learn research methods/hacks they didn't already know (makes their progress faster so the size of the gap decreases in growth rate).
- Consider the little time we might get to deploy these complex mechanisms nationally (the SF AI people thought about this way more and will take gov officials lots more time to understand which might be too late).

**AGI investment firm (w/ Stripe CEO Patrick Collinson & friends)**

- Why is this the thing to do? -> Massive investment gain (fastest growing technology that scales itself like nothing else), being in a position to advise (push things in the right direction), personal freedom, sequence of bets on the way to AGI, experience navigating the "the unknowns in the unknowns" quadrant.
- Coming from an econ background, it's good to take in the seriousness in Leopold saying the gains are going to scale far more than modern financial markets expect.
- Still early to invest but make sure you know the logistics of how compute/energy/talent is being drawn.
- Criticizing e/accs by shitposting about superintel/AGI stuff without thinking it through long-term (security aspects, writing stuff down).

## **Superalignment**

- After some careful thinking, it seems the real problem of "superalignment" is understanding what your own long-term goal is. This is an impossibly hard problem because we don't know what will happen in 50-100 years from now. We can provide human assistance to gain the intellectual benefit but also have the slight human "push" in case things don't go according to plan. The issue with this is the limit of humans. We don't actually know what the moral solutions are all the time (given by things like the trolley problem). We don't know the long-term bloating or cons of our decisions in the extreme long-term (thousands of years) and could end up in a place where we aren't moral enough to be held responsible for making such decisions. Out of desperation, we may request a superintelligence to guide us. We are now in a place where decisions and morality are outsourced to the entities we didn't initially intend to make for us. If we have a slow takeoff, we should be fine because the difficulty of morally complex problems should scale at roughly the rate of intelligence (this is uncertain and vague).
- The real issues don't kick in unless we are giving these blackbox AIs to the public for relatively cheap compute. People will always find a way to jailbreak them and we may have a bunch of mini cyber attacks everyday.
- Tools to evaluation what its thinking out loud vs silently (without directly saying it).
- The more risk, the more the reward. Deploy at low risks (limited modalities if possible) and use public interaction/data to improve alignment research (miniscule alignment bugs).
- The main concern from these articles resides in the "AGI to Superintelligence" risk. Just don't scale it as quickly. The only condition which you would have to scale as fast as possible would be if China has your weights. This is likely to happen so we have to find a solution that limits what both parties can do (or maybe there will be a cybersec breakthrough that forces China to steal research rather than weights).
- First of all, how would we even know if we are at "AGI"? The objective gap people have drawn seems ridiculous. These are dependent on the evals, the authors of the evals, and a bunch of surrounding checkboxes we don't even know exist. In reality, I think the models will keep getting better and better until they start to surpass human intel and become a security threat if the energy infrastructure can support all our "helpers."
- OpenAI's superalignment question resides in taking the relative intelligence of models to supervise each other. If GPT-2 can safely supervise GPT-4, then you continue scaling up. The obvious issue I see here is being able to trust that GPT-6 was properly supervised by GPT-4 and is thus ready to supervise GPT-8.
- Mech interp is a promising approach to understanding the internals of an architecture such that we completely understand the internals.

---

## **GPT-4 to AGI**

### **Copy paste directly from article chapter 1**

In addition to insider bullishness, I think there’s a strong intuitive case for why it should be possible to find ways to train models with much better sample efficiency (algorithmic improvements that let them learn more from limited data). Consider how you or I would learn from a really dense math textbook:

What a modern LLM does during training is, essentially, very very quickly skim the textbook, the words just flying by, not spending much brain power on it.

Rather, when you or I read that math textbook, we read a couple pages slowly; then have an internal monologue about the material in our heads and talk about it with a few study-buddies; read another page or two; then try some practice problems, fail, try them again in a different way, get some feedback on those problems, try again until we get a problem right; and so on, until eventually the material “clicks.”

You or I also wouldn’t learn much at all from a pass through a dense math textbook if all we could do was breeze through it like LLMs.

But perhaps, then, there are ways to incorporate aspects of how humans would digest a dense math textbook to let the models learn much more from limited data. In a simplified sense, this sort of thing—having an internal monologue about material, having a discussion with a study-buddy, trying and failing at problems until it clicks—is what many synthetic data/self-play/RL approaches are trying to do.

Major breakthroughs are in RLHF and variants, chain of thought reasoning to solve math & SWE problems (walking through the steps until the answer is achieved works better because it enables more useful information flow each token gen / forward pass, scaffolding -> a bunch of specialized models solving a problem in few-shot settings rather than GPT-4 figuring it out the first try. Far less compute used from this last approach and enabled more capabilities when we optimize it further. Tokenization hacks -> query tools like calculators. More compute efficient algos provides us with longer ctx length. Really hard evals lead us to unconventional thinking which lead us to non-trivial answers that would otherwise be far out.

---

## **Features for the futures:**

- Working memory/replay buffer to store long term relationships/dependencies about a client's needs.
- Recurrent architectures to digest a problem thoroughly instead of dumbly producing the next token based on how a bunch of internal features attended to one another. Could use a scaffolding variant here but having an architectural change might scale better. Optimize for longer-horizon problems.
- A good research project would be looking at how many useful "attentions" (the spectrum of useful attending of tokens/features attending to others) in each layer. Connect this to other hyperparameters and architectural changes to see what we can add/change about the base self-attention algorithm to get more usefulness out of all its parameters.
- System 1 & 2 rather than just system 1.

---

## **Threats:**

- The central problem here is security rather than unpredictability/exploitability of the largest models.
- … Is this what we see at OpenAI or any other American AI lab? No. In fact, what we see is the opposite — the security equivalent of swiss cheese. Chinese penetration of these labs would be trivially easy using any number of industrial espionage methods, such as simply bribing the cleaning crew to stick USB dongles into laptops. My own assumption is that all such American AI labs are fully penetrated and that China is getting nightly downloads of all American AI research and code RIGHT NOW… -> Marc Andreessen
- Obvious but good to remind ourselves about protecting model weights (the destination) & the algorithmic secrets required to obtain the end weights via pretraining, finetuning, RLHF, etc.
- CCP gaining advantage of holding the technology that allows them to keep up via automation, all through espionage.
- Not only would it present a national security risk but being in a superintelligence race (naive: whoever gains it first gets absolute control over the other (AGI in a box problem)) but it means little to no margin of safety in development of these systems. This means the mech interp techniques would need to improve a lot in order to understand potentially dangerous internal behavior.

### **Random Notes:**

- Best to think in OOMs (orders of magnitude) rather than 2x, 6x, 13x, etc.
- Sure, recursively improving general intelligence or whatever people call it. I'd like to question the takeoff speed and practicality here. The # of agents doing the research to improve themselves would cost an amount per agent or per task. This would be an exponential takeoff with a very explicit limit -> cost, energy, & climate. Having an agent smart enough to test would take a while to actually build & test. Assuming this is autoregressive, it would fail a lot at generating useful code and having the creativity we humans do. You would need the smartest people in the world giving feedback constantly. Leopold has strictly stuck to the obvious advancements (where the hype is at) in AI so having this expected high-quality data with some sort of RLHF mechanism to teach a model to build itself would take a while to implement and scale. Not out of possibility but there will surely be some missing pieces here holding us back from the pot of gold.
- We only have a limited amount of data from existing trained models but people mainly compare GPT-2 to GPT-4 in terms of performance change over time. We need to understand whats going on under the hood to draw such concise general intelligence predictions.
- There needs to be clear instructions on how to actually conduct AI research.
- It could be possible that without knowing, Leopold handpicked examples to make all this progress seem like it's "just around the corner" when there are other challenges and really hard problems in edge cases.
- Aluminum smelting companies for the energy requirements handling the compute requirements this decade. Could buy into the ETFs supporting energy infrastructure (look into advancements in the field to see where most infrastructural growth will occur).
- Since building up power plants and giga clusters in other parts of the world could be faster, we have to consider the influence they have over research in the US. They can peek into the weights or training run at any point if they are careful enough and this poses a national security risk. This then jumps to a climate issue if we keep energy + cluster development in the US due to pollution from our #1 energy source of gasoline power plants.
- Might be smarter for long-term personal financial investment in compute manufacturing (UV lithography) / energy bottlenecks because research and innovation will kick that right out of place leaving massive gains for those early invested.
- Since you have a national security threat, all the top end research would fall into the hands of military and government control, siphoning power from the secretive private companies. I wonder if really good corp structure could resolve some of this.
- It's healthy to consider the possible capability limitations of hacks like FP8 quantization could force the model to learn to adapt around low precision and thus prevent it from focusing more parameters on the proclaimed "intelligence" part of the model. We may discover that the true issues lay at the maximally useful weights in the neural net.
- I have heard from a number of researchers that the research conducted in big tech is often "compute limited". I don't know why a single consumer-grade GPU or even an A100 isn't enough for a ton of algorithmic progress. Of course the scaling experiments require more compute but the base hacks like layernorm and residual connections can be reproduced on many scales w/ 4GB of VRAM.
- Scaling laws essentially says you can determine the exact loss to a very high precision based on the total training tokens and model size (VRAM & data amount (assuming a quality threshold)).

---

## **Other Resources**

- GPT scaling laws (Chinchilla) -> [**Link**](https://arxiv.org/pdf/2203.15556)
- Original scaling laws w/ Dario Amodei -> [**Link**](https://arxiv.org/pdf/2001.08361)
- Super thorough scaling paper I probably won't read -> [**Link**](https://arxiv.org/pdf/2112.11446)
